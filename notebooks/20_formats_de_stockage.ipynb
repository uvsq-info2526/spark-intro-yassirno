{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialisation de Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/11/10 15:35:44 WARN Utils: Your hostname, Dell, resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/11/10 15:35:44 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/10 15:35:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/11/10 15:35:45 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Exemples avec les formats de stockage\") \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chargement des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement des prénoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------+-----+---+--------+\n",
      "|sexe|        prenom|annee|dep|effectif|\n",
      "+----+--------------+-----+---+--------+\n",
      "|   1|_PRENOMS_RARES| 1910| 06|      38|\n",
      "|   1|_PRENOMS_RARES| 1919|974|     122|\n",
      "|   1|_PRENOMS_RARES| 1921| 84|       3|\n",
      "|   1|_PRENOMS_RARES| 1929| 43|       7|\n",
      "|   1|_PRENOMS_RARES| 1934|971|     190|\n",
      "|   1|_PRENOMS_RARES| 1942| 85|      11|\n",
      "|   1|_PRENOMS_RARES| 1949| 34|      10|\n",
      "|   1|_PRENOMS_RARES| 1967| 04|       3|\n",
      "|   1|_PRENOMS_RARES| 1967| 08|      17|\n",
      "|   1|_PRENOMS_RARES| 1992| 75|    1166|\n",
      "|   1|_PRENOMS_RARES| 1995| 32|       4|\n",
      "|   1|_PRENOMS_RARES| 1998| 82|      19|\n",
      "|   1|_PRENOMS_RARES| 2011| 57|     210|\n",
      "|   1|_PRENOMS_RARES| 2016| 29|     158|\n",
      "|   1|      ABDALLAH| 1967| 78|       3|\n",
      "|   1|    ABDELHAFID| 1968| 59|       3|\n",
      "|   1|    ABDELKADER| 1965| 71|       3|\n",
      "|   1|    ABDELKADER| 1975| 77|       8|\n",
      "|   1|    ABDELKADER| 1982| 71|       5|\n",
      "|   1|    ABDELKARIM| 1985| 75|       4|\n",
      "+----+--------------+-----+---+--------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "# Charge un fichier texte et convertit les lignes en \"Row\".\n",
    "lines = sc.textFile(\"prenoms_sample.txt\")\n",
    "prenoms_as_rdd = lines.map(lambda l: l.split(\";\"))\\\n",
    "    .map(lambda p: Row(\\\n",
    "        sexe=int(p[0]),\\\n",
    "        prenom=p[1],\\\n",
    "        annee=int(p[2]),\\\n",
    "        dep=p[3],\\\n",
    "        effectif=int(p[4])))\n",
    "\n",
    "# Infère le schéma et enregistre le DataFrame comme une table.\n",
    "prenoms = spark.createDataFrame(prenoms_as_rdd)\n",
    "prenoms.createOrReplaceTempView(\"prenoms\")\n",
    "prenoms.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement des départements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+--------+----+--------------------+--------------------+--------------------+\n",
      "|dep|reg|cheflieu|tncc|                 ncc|              nccenr|             libelle|\n",
      "+---+---+--------+----+--------------------+--------------------+--------------------+\n",
      "|  1| 84|   01053|   5|                 AIN|                 Ain|                 Ain|\n",
      "|  2| 32|   02408|   5|               AISNE|               Aisne|               Aisne|\n",
      "|  3| 84|   03190|   5|              ALLIER|              Allier|              Allier|\n",
      "|  4| 93|   04070|   4|ALPES DE HAUTE PR...|Alpes-de-Haute-Pr...|Alpes-de-Haute-Pr...|\n",
      "|  5| 93|   05061|   4|        HAUTES ALPES|        Hautes-Alpes|        Hautes-Alpes|\n",
      "|  6| 93|   06088|   4|     ALPES MARITIMES|     Alpes-Maritimes|     Alpes-Maritimes|\n",
      "|  7| 84|   07186|   5|             ARDECHE|             Ardèche|             Ardèche|\n",
      "|  8| 44|   08105|   4|            ARDENNES|            Ardennes|            Ardennes|\n",
      "|  9| 76|   09122|   5|              ARIEGE|              Ariège|              Ariège|\n",
      "| 10| 44|   10387|   5|                AUBE|                Aube|                Aube|\n",
      "| 11| 76|   11069|   5|                AUDE|                Aude|                Aude|\n",
      "| 12| 76|   12202|   5|             AVEYRON|             Aveyron|             Aveyron|\n",
      "| 13| 93|   13055|   4|    BOUCHES DU RHONE|    Bouches-du-Rhône|    Bouches-du-Rhône|\n",
      "| 14| 28|   14118|   2|            CALVADOS|            Calvados|            Calvados|\n",
      "| 15| 84|   15014|   2|              CANTAL|              Cantal|              Cantal|\n",
      "| 16| 75|   16015|   3|            CHARENTE|            Charente|            Charente|\n",
      "| 17| 75|   17300|   3|   CHARENTE MARITIME|   Charente-Maritime|   Charente-Maritime|\n",
      "| 18| 24|   18033|   2|                CHER|                Cher|                Cher|\n",
      "| 19| 75|   19272|   3|             CORREZE|             Corrèze|             Corrèze|\n",
      "| 21| 27|   21231|   3|           COTE D OR|           Côte-d'Or|           Côte-d'Or|\n",
      "+---+---+--------+----+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "lines = sc.textFile(\"dpts.txt\")\n",
    "depts_as_rdd = lines\\\n",
    "    .filter(lambda l: \"dep\" not in l and \"2A\" not in l and \"2B\" not in l)\\\n",
    "    .map(lambda l: l.split(\",\"))\\\n",
    "    .map(lambda p: Row(\\\n",
    "        dep=int(p[0]),\\\n",
    "        reg=int(p[1]),\\\n",
    "        cheflieu=p[2],\\\n",
    "        tncc=p[3],\\\n",
    "        ncc=p[4],\\\n",
    "        nccenr=p[6],\\\n",
    "        libelle=p[6]))\n",
    "\n",
    "depts = spark.createDataFrame(depts_as_rdd)\n",
    "depts.createOrReplaceTempView(\"depts\")\n",
    "depts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sauvegarder les données au format parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Les prénoms partitionnés par départements et années et compressés (Snappy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "prenoms.write\\\n",
    "    .partitionBy('dep', 'annee')\\\n",
    "        .format('parquet')\\\n",
    "            .save('prenomsParDeptsEtAnnees.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Les prénoms partitionnés par départements et années et compressés (gzip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "prenoms.write\\\n",
    "    .partitionBy('dep', 'annee')\\\n",
    "        .option(\"compression\", \"gzip\")\\\n",
    "            .format('parquet')\\\n",
    "                .save(\"prenomsParDeptsEtAnnees.gzip.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Les départements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depts.write\\\n",
    "    .format(\"parquet\")\\\n",
    "        .save(\"depts.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sparkenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
